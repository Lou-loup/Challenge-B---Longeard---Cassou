---
title: "CHALLENGE B"
author: "Cassou Louise - Longeard Lou - https://github.com/Lou-loup/Challenge-B---Longeard---Cassou.git" 
output: html_document
---
##_**TASK 1B - PREDICTING HOUSE PRICES IN AMES, IOWA (continued)**_

##Step 1

We decide to use the random forests ML technique. Random Forest is a machine learning algorithm that is particularly effective in identifying links between a variable to be explained and explanatory variables. Random Forest will classify the explanatory variables according to their links with the variable to be explained.
Random Forest makes random regressions taking randomly explanatory variables in a data set.

```{r, include=FALSE, echo = FALSE}
library(tidyverse)
library(np)
library(randomForest)
training <- read.csv("train.csv")
test <- read.csv("test.csv")
```

```{r, echo= TRUE, include=FALSE}
training2 <- select(training, -Id)
```

```{r missing data 2, echo= FALSE, include=FALSE}
remove.vars <- training2 %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 100) %>% select(feature) %>% unlist

training2 <- training2 %>% select(- one_of(remove.vars))

```

```{r missing data 3, echo= FALSE, include=FALSE}

training2 %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

training2 <- training2 %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)

training2 %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

```

```{r housing-step9-sol, echo = FALSE, include=FALSE}
cat_var <- training2 %>% summarise_all(.funs = funs(is.character(.))) %>% gather(key = "feature", value = "is.chr") %>% filter(is.chr == TRUE) %>% select(feature) %>% unlist

training2 %>% mutate_at(.cols = cat_var, .funs = as.factor)
```

##Step 2
```{r Step 2.1, include=TRUE,echo=TRUE}
set.seed(1)
training_RF <- randomForest(SalePrice~., data=training2, ntree=300, mtry=8, na.action = na.roughfix)
print(training_RF)
```

Thanks to the command select we discard the variable Id as a feature. Then, we use randomForest command to make 300 different regressions with 8 random features at each regression.
This method is good because 86.26 % of the variations of the Sale Price is explained by the model.

##Step 3

```{r Step 3.1, include=TRUE,echo=TRUE}
prediction2 <- predict(training_RF, data = test, type="response")
summary(prediction2)
```

```{r Step 3.1.1, include=TRUE,echo=TRUE}
lm_model_2 <- lm(SalePrice ~ MSZoning + LotArea + Neighborhood  + YearBuilt + OverallQual, data = training2)
prediction1 <- data.frame(Id = test$Id, SalePrice_predict = predict(lm_model_2, test, type="response"))
summary(prediction1)
```
We make a prediction of SalePrice thanks to the RandomForest technique on the test data. We compare these predictions to the predictions of a linear regression from Challenge A. This prediction is done thanks to the linear regression of SalePrice on MSZoning, LotArea, Neighborhood, YearBuilt, OverallQual.
The mean and median are rather the same. But in the RandomForest prediction the minimum of the price is higher than the one of the linear regression prediction. With the linear regression technique the minimum sale price predicted is equal to 11634$ wich is not realistic case. 
In the RandomForest prediction the maximum of the price is higher than the one of the linear regression prediction. Even if it is higher we think that it can be a realistic case.




##_**TASK 2B - OVERFITTING IN MACHINE LEARNING (continued)**_

```{r overfit, echo = FALSE, include = FALSE}
rm(list = ls())

library(tidyverse)
library(np)
library(caret)

set.seed(1)
Nsim <- 150
b <- c(0,1)
x0 <- rep(1, Nsim)
x1 <- rnorm(n = Nsim)

X <- cbind(x0, x1^3)
y.true <- X %*% b

eps <- rnorm(n = Nsim)
y <- X %*% b + eps

df <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value)



ggplot(df) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true))

training.index <- createDataPartition(y = y, times = 1, p = 0.8)
df <- df %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "training", "test"))

training <- df %>% filter(which.data == "training")
test <- df %>% filter(which.data == "test")


lm.fit <- lm(y ~ x, data = training)
summary(lm.fit)

training.index <- createDataPartition(y = y, times = 1, p = 0.8)
df <- df %>% mutate(y.lm = predict(object = lm.fit, newdata = df))
training <- training %>% mutate(y.lm = predict(object = lm.fit))
test <- df %>% filter(which.data == "test")

lm.fit <- lm(y ~ x, data = training) 
summary(lm.fit)

training <- training %>% mutate(y.lm = predict(object = lm.fit))
```

##Step 1

```{r step 1, echo=TRUE}
ll.fit.lowflex <- npreg(y ~ x, data = training, method = "ll", bws = 0.5)
summary(ll.fit.lowflex)
```
For this question, we use the npreg function.It computes a kernel regression estimate. Thanks to the ll method, we can make different local regressions with a bandwidth of 0.5

##Step 2
```{r step 2, echo=TRUE}
ll.fit.highflex <- npreg(y ~ x, data = training, method = "ll", bws = 0.01)
summary(ll.fit.highflex)
```
We do the same as in step 1 but we choose a bandwidth of 0.01. Like that, we can make more regression. 

##Step 3
```{r step 3, echo=TRUE}

df <- df %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = df), y.ll.highflex = predict(object = ll.fit.highflex, newdata = df))
training <- training %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = training), y.ll.highflex = predict(object = ll.fit.highflex, newdata = training))

ggplot(training) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true))+
  geom_line(data=training, aes(x=x, y=y.ll.highflex), col="blue")+
  geom_line(data=training, aes(x=x, y=y.ll.lowflex), col="red")+
  ggtitle("Predictions of ll.fit.lowflex and ll.fit.highflex on training data")

```


First, thanks to the predict function we predict the values of our local regressions. Then, we create a data frame with all predicted values. We called it df. We give a new value to the training data set that include our predictions but only for the training values.
Then, we plot in blue the values from the prediction of ll.fit.highflex and in red the ones of ll.fit.lowflex.

##Step 4
```{r step 4, echo=TRUE}
summary(training)
```
Thanks to the graph and the summary of training, we can see that the prediction of the high-flexibility local linear model is more variable. It can be explained by the fact that the bandwidth is smaller. 
The high-flexibility local linear model predictions have the least bias. Indeed, the minimum value of y.ll.hiflex is the same as the real minimmum value of y (-9.9764). It is the same for the maximum value (13.6876). In fact, the bandwidth is small so the slop of the regressions change at each step. And it can follow better the real shape the real model. 

##Step 5
```{r step 5, echo=TRUE}
test <- test %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = test), y.ll.highflex = predict(object = ll.fit.highflex, newdata = test))

ggplot(test) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true))+
  geom_line(data=test, aes(x=x, y=y.ll.highflex), col="blue")+
  geom_line(data=test, aes(x=x, y=y.ll.lowflex), col="red")+
  ggtitle("Predictions of ll.fit.lowflex and ll.fit.highflex on test data")

summary(test)

```
First, we use our df data frame to give a new value to the test data set that include our predictions but only for the test values.
Then, we plot in blue the values from the prediction of ll.fit.highflex and in red the ones of ll.fit.lowflex.
We can see that the predictions of the high-flexibility local linear model is more variable because the bandwidth is smaller. 
But in this case, the low-flexibility local linear model predictions have the least bias. In fact, the bandwidth is small and the test data frame does not contain a lot of values. It makes different regressions but in some bandwidth there is no data so the slope has a bias.

##Step 6
```{r step 6, echo=TRUE}
bdw <- seq(0.01, 0.5, by = 0.001)
```
Thanks to the command seq, we create a vector of bandwidth going from 0.01 to 0.5 with a step of 0.001. We call it bdw.

##Step 7
```{r step 7, echo=TRUE}
ll.bdw.fit <- lapply(X = bdw, FUN = function(bdw) {npreg(y ~ x, data = training, method = "ll", bws = bdw)})
```
We use the lapply command which takes a vector and gives back a list of local regressions. We do it on ou training data frame. We call it ll.bdw.fit. 

##Step 8
```{r step 8, echo=TRUE}
mse.training <- function(fit.model){
  predictions <- predict(object = fit.model, newdata = training)
  training %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}
mse.train.results <- unlist(lapply(X = ll.bdw.fit, FUN = mse.training))
```
We create a function mse.training which take the value of the mean square error function. Then, we apply this function to the values of our list of local linear regressions (ll.bdw.fit). We can not print our results because there is too many values (step of 0.001). It gives the MSE for all local regression on our training data frame. 

##Step 9
```{r step 9, echo=TRUE}
mse.test <- function(fit.model){
  predictions <- predict(object = fit.model, newdata = test)
  test %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}
mse.test.results <- unlist(lapply(X = ll.bdw.fit, FUN = mse.test))
```
We do the same as in step 8 but on our test data. As in step 8, we cannot print our results because there are too many MSE for each local regression on our test data frame.

##Step 10
```{r step 10, echo=TRUE}
mse.df <- tbl_df(data.frame(bandwidth = bdw, mse.train = mse.train.results, mse.test = mse.test.results))

ggplot(mse.df)+geom_line(mapping=aes(x=bdw, y=mse.train), color="blue")+
  geom_line(mapping=aes(x=bdw, y=mse.test), color="orange")+
  ggtitle("MSE on training and test data for different bandwidth - local linear regression")
```


We create a data frame of our MSE found in step 8 and 9. Thanks to this new data frame, we can plot the MSE of the training data and the MSE of the test data on the same plot regarding the size of the bandwidth. We plot the MSE of our test data in orange in the MSE of our training data in blue.

The data set train has many observations so small bandwidth implies low mean squared errors. It makes several regression with small bandwidth so it is close to the real model at each regression. As bandwidth increases the mean squared error increases. Regressions have higher bandwidth so it is less close to the real model.

The data set test has a small number of observations so small bandwidth implies high mean squared errors. It makes several regressions with small bandwidth but it does not have enough obersvations to be close to the real model.So the mean squared error decreases. When the bandwidth increases the number of observations for each bandwith is higher so the regression is better. But when the bandwidth beacomes too high then the mean squared error increases.


##_**TASK 3B - PRIVACY REGULATION COMPLIANCE IN FRANCE**_

```{r step 1.3, include=FALSE, echo=TRUE}
cnil<-read.csv(file=file.choose(), header=TRUE, sep=";", dec=",", stringsAsFactors=FALSE)
```

```{r step 2.3, include=TRUE, echo=TRUE}
library(knitr)
cnil2 <- subset(cnil, nchar(cnil$Code_Postal) > 4,)

cnil3 <- subset(cnil2, nchar(cnil2$Code_Postal) < 6,)

cp <- sub ("^(\\d{2}).*$", "\\1", cnil3$Code_Postal);cp2 <- subset(cp, nchar(cp) < 3,)

table1<-data.frame(table(unlist(cp2)))
colnames(table1)[colnames(table1)=="Var1"] <- "Departement"
colnames(table1)[colnames(table1)=="Freq"] <- "Number of organizations"

table1
```

Thanks to the command subset, we select the postal codes which have more than 4 numbers and then, we apply again this command to select those with less than 6 numbers. So we have only postal codes with 5 numbers at the end.
Then, we use the command sub to select only the 2 first numbers of the postal code. 
Finally, we create a data frame with this postal code and the number of organization that has nominated a CNIL for each one.
But, for the department 97 and 98 we notice that there are more than one department into it because overseas department's postal code have more than 2 numbers.